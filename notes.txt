 @app.route('/upload', methods=['GET', 'POST'])
# def upload_file():
#     if request.method == 'POST':
#         if 'file' not in request.files:
#             return 'No audio file selected!'

#         file = request.files['file']

#         if file.filename == '':
#             return 'No selected file!'

#         if file:
#             filename = secure_filename(file.filename)

#             # Optional: Save audio file locally for testing (adjust path)
#             file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
#             file.save(file_path)
#             print(file_path)
#             file.seek(0)

#             # Store audio file in GridFS
#             audio_data = file.read()  # Read audio data
#             audio_id = fs.put(audio_data, filename=filename)  # Store in GridFS

#             # Collect agent information
#             agent_name = request.form['agent_name']
#             agent_id = request.form['agent_id']

#             # Create a dictionary to store agent data and audio ID
#             agent_data = {
#                 "agent_name": agent_name,
#                 "agent_id": agent_id,
#                 "audio_id": audio_id,
#             }

#             # Insert agent data into MongoDB (assuming a collection named 'Agent')
#             db.calls.insert_one(agent_data)
#             # return redirect(url_for('agents'))
#             return process_audio(file_path)

#             # return f'Recording uploaded successfully for {agent_name}! Filename: {filename}'
            
#     else:
#         return render_template('upload.html')





# @app.route('/transcribe', methods=['POST','GET'])
# def transcribe():
#     audio_path = request.form['audio_path']
#     search_words = request.form.get('search_words', '').split(',')
#     search_words = [word.strip() for word in search_words]

#     # Transcribe concatenated audio
#     concatenated_transcriptions = {}
#     for speaker_index in [0, 1]:  # Assuming 2 speakers
#         concatenated_filename = f"{app.config['UPLOAD_FOLDER']}/concatenated_speaker_{speaker_index}.wav"
#         concatenated_transcriptions[speaker_index] = transcribe_file(concatenated_filename)

#     highlighted_transcriptions = {}
#     for speaker_index, transcription in concatenated_transcriptions.items():
#         highlighted_text = []
#         for segment in transcription:
#             highlighted_text.append(highlight_text(segment['text'], search_words))
#         highlighted_transcriptions[speaker_index] = highlighted_text

#     return jsonify(highlighted_transcriptions)

# def rttm_to_dataframe(rttm_file_path):
#     columns = ["Type", "File ID", "Channel", "Start Time", "Duration", "Orthography", "Confidence", "Speaker", 'x', 'y']
#     with open(rttm_file_path, "r") as rttm_file:
#         lines = rttm_file.readlines()
#     data = []
#     for line in lines:
#         data.append(line.strip().split())
#     df = pd.DataFrame(data, columns=columns)
#     df = df.drop(['x', 'y', "Orthography", "Confidence"], axis=1)
#     return df

# def highlight_text(text, search_words):
#     highlighted_text = text
#     for word in search_words:
#         regex = re.compile(re.escape(word), re.IGNORECASE)
#         highlighted_text = regex.sub(f"<mark>{word}</mark>", highlighted_text)
#     return highlighted_text

# def transcribe_file(audio_file):
#     if not os.path.exists(audio_file):
#         print(f"File {audio_file} not found!")
#         return []
#     audio = whisperx.load_audio(audio_file)
#     result = whisper_model.transcribe(audio, batch_size=batch_size)
#     return result["segments"]




# @app.route('/transcribe', methods=['POST', 'GET'])
# def transcribe():
#     try:
#         # Get the audio_id from form or URL parameters
#         audio_id = request.form.get('id') or request.args.get('id')
#         audio_data = fs.get(ObjectId(audio_id))
#         if audio_data is None:
#             return jsonify({'error': 'Audio not found'}), 404

#         audio_binary = audio_data.read()
#         audio_path = os.path.join(app.config['UPLOAD_FOLDER'], audio_data.filename)
#         with open(audio_path, 'wb') as f:
#             f.write(audio_binary)

#         search_words = request.form.get('search_words', '').split(',')
#         search_words = [word.strip() for word in search_words]

#         # Transcribe concatenated audio
#         concatenated_transcriptions = {}
#         for speaker_index in [0, 1]:  # Assuming 2 speakers
#             concatenated_filename = os.path.join(app.config['UPLOAD_FOLDER'], f"concatenated_speaker_{speaker_index}.wav")
#             concatenated_transcriptions[speaker_index] = transcribe_file(concatenated_filename)

#         highlighted_transcriptions = {}
#         for speaker_index, transcription in concatenated_transcriptions.items():
#             highlighted_text = []
#             for segment in transcription:
#                 highlighted_text.append(highlight_text(segment['text'], search_words))
#             highlighted_transcriptions[speaker_index] = highlighted_text

#         return render_template('transcribe.html', transcriptions=highlighted_transcriptions)

#     except Exception as e:
#         return jsonify({'error': str(e)}), 500

# def rttm_to_dataframe(rttm_file_path):
#     columns = ["Type", "File ID", "Channel", "Start Time", "Duration", "Orthography", "Confidence", "Speaker", 'x', 'y']
#     with open(rttm_file_path, "r") as rttm_file:
#         lines = rttm_file.readlines()
#     data = []
#     for line in lines:
#         data.append(line.strip().split())
#     df = pd.DataFrame(data, columns=columns)
#     df = df.drop(['x', 'y', "Orthography", "Confidence"], axis=1)
#     return df

# def highlight_text(text, search_words):
#     highlighted_text = text
#     for word in search_words:
#         regex = re.compile(re.escape(word), re.IGNORECASE)
#         highlighted_text = regex.sub(f"<mark>{word}</mark>", highlighted_text)
#     return highlighted_text

# def transcribe_file(audio_file):
#     if not os.path.exists(audio_file):
#         print(f"File {audio_file} not found!")
#         return []
#     audio = whisperx.load_audio(audio_file)
#     result = whisper_model.transcribe(audio, batch_size=batch_size)
#     return result["segments"]



# @app.route('/predict', methods=['GET'])
# def predict():
#     try:
#         audio_id = request.args.get('id')
#         audio_data = fs.get(ObjectId(audio_id))
#         if audio_data is None:
#             return jsonify({'error': 'Audio not found'}), 404

#         audio_binary = audio_data.read()
#         audio_path = os.path.join(app.config['UPLOAD_FOLDER'], audio_data.filename)
#         with open(audio_path, 'wb') as f:
#             f.write(audio_binary)

#         df, df_speaker_00, df_speaker_01 = process_audio(audio_path)

#         # Plot for Speaker 01
#         plt.figure(figsize=(10, 6))
#         plt.plot(df_speaker_01['Start Time'], df_speaker_01['End Time'], marker='o')
#         plt.xlabel('Start Time')
#         plt.ylabel('End Time')
#         plt.title('End Time vs Start Time with Emotion (Speaker 01)')
#         plt.grid(True)
#         for i, row in df_speaker_01.iterrows():
#             plt.annotate(row['emotion'], (row['Start Time'], row['End Time']), textcoords="offset points", xytext=(0, 10), ha='center')

#         img_speaker_01 = io.BytesIO()
#         plt.savefig(img_speaker_01, format='png')
#         img_speaker_01.seek(0)
#         plot_url_speaker_01 = base64.b64encode(img_speaker_01.getvalue()).decode()

#         # Plot for Speaker 00
#         plt.figure(figsize=(10, 6))
#         plt.plot(df_speaker_00['Start Time'], df_speaker_00['End Time'], marker='x')
#         plt.xlabel('Start Time')
#         plt.ylabel('End Time')
#         plt.title('End Time vs Start Time with Emotion (Speaker 00)')
#         plt.grid(True)
#         for i, row in df_speaker_00.iterrows():
#             plt.annotate(row['emotion'], (row['Start Time'], row['End Time']), textcoords="offset points", xytext=(0, 10), ha='center')

#         img_speaker_00 = io.BytesIO()
#         plt.savefig(img_speaker_00, format='png')
#         img_speaker_00.seek(0)
#         plot_url_speaker_00 = base64.b64encode(img_speaker_00.getvalue()).decode()

#         return render_template("results.html", 
#                                tables=df.to_html(classes='data'), 
#                                df_speaker_00=df_speaker_00.to_html(classes='data'), 
#                                df_speaker_01=df_speaker_01.to_html(classes='data'), 
#                                plot_url_speaker_01=plot_url_speaker_01,
#                                plot_url_speaker_00=plot_url_speaker_00)
#     except Exception as e:
#         return jsonify({'error': str(e)}), 500

    66759a682d0be614df99733c
    66759a682d0be614df99733c
    http://127.0.0.1:5000/transcribe?id=66759a682d0be614df99733c


    <iframe title="ddfinallll" width="600" height="373.5" src="https://app.powerbi.com/view?r=eyJrIjoiODBiMTQ4YjktNmE3NS00YzhjLThjZWMtOGFiODIyZWY0YjUyIiwidCI6ImVhZjYyNGM4LWEwYzQtNDE5NS04N2QyLTQ0M2U1ZDc1MTZjZCIsImMiOjh9" frameborder="0" allowFullScreen="true"></iframe>

    <iframe title="Overview" width="600" height="373.5" src="https://app.powerbi.com/view?r=eyJrIjoiMTY4NTNmNWQtNDY2YS00ZmVjLTgzNDUtYTNjYjBjZjllNjU5IiwidCI6ImVhZjYyNGM4LWEwYzQtNDE5NS04N2QyLTQ0M2U1ZDc1MTZjZCIsImMiOjh9" frameborder="0" allowFullScreen="true"></iframe>

    <iframe title="Calls_Overview" width="600" height="373.5" src="https://app.powerbi.com/view?r=eyJrIjoiNjA5MTAwYmYtOGM3ZS00YjlmLWI5YWEtMWVhNmU1OTE3NzFjIiwidCI6ImVhZjYyNGM4LWEwYzQtNDE5NS04N2QyLTQ0M2U1ZDc1MTZjZCIsImMiOjh9" frameborder="0" allowFullScreen="true"></iframe>

    <iframe title="Calls-Agents" width="600" height="373.5" src="https://app.powerbi.com/view?r=eyJrIjoiMzI5OGMzMmItZTY5Ny00YWZjLThlNzYtM2M1MzQ0YTJiNDk1IiwidCI6ImVhZjYyNGM4LWEwYzQtNDE5NS04N2QyLTQ0M2U1ZDc1MTZjZCIsImMiOjh9" frameborder="0" allowFullScreen="true"></iframe>